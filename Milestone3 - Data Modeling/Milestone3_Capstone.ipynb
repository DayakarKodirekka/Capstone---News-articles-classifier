{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Milestone3_Capstone",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4dmyM-dzang",
        "outputId": "7a6f3f29-bfa4-467b-e765-801fa1334691"
      },
      "source": [
        "!pip install mlflow --quiet\n",
        "import mlflow\n",
        "print(mlflow.__version__)\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 14.6 MB 115 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 40.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 146 kB 51.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 58.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "1.20.2\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 43 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 57.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=d17b9a48bdb669badb94f0d12e975a9b813ae75cde778015d6e1e8968b179c79\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-4QcFBMzkbq",
        "outputId": "07608e2b-d085-465e-f7e1-a75075ac235a"
      },
      "source": [
        "!pip install pyngrok --quiet\n",
        "from pyngrok import ngrok\n",
        "from getpass import getpass\n",
        "# Terminate open tunnels if any exist\n",
        "# ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 481 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 716 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 745 kB 7.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1E-XIVx1kId"
      },
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "import mlflow\n",
        "import mlflow.spark\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSOJwUUq1pdj"
      },
      "source": [
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WdEDodakmor"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYz4wTpu1sQ5"
      },
      "source": [
        "df = pd.read_csv('newsF.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnKs65aX1ukz"
      },
      "source": [
        "train_df = spark.createDataFrame(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPYk9-6i0Q0S",
        "outputId": "23da9c7b-3651-4614-f738-b08c25e87edc"
      },
      "source": [
        "# Start a new MLflow run\n",
        "import mlflow.mleap\n",
        "import mlflow.sklearn\n",
        "with mlflow.start_run(run_name=\"News_priediction\") as run:\n",
        "    # regular expression tokenizer\n",
        "  regexTokenizer = RegexTokenizer(inputCol=\"summary\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "  # stop words\n",
        "  add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
        "  stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
        "  # bag of words count\n",
        "  countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
        "  label_stringIdx = StringIndexer(inputCol = \"topic\", outputCol = \"label\")\n",
        "  pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "  # Fit the pipeline to training documents.\n",
        "  pipelineFit = pipeline.fit(train_df)\n",
        "  dataset = pipelineFit.transform(train_df)\n",
        "  dataset.show(5)\n",
        "  # set seed for reproducibility\n",
        "  (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "  print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "  print(\"Test Dataset Count: \" + str(testData.count()))\n",
        "  lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "  lrModel = lr.fit(trainingData)\n",
        "  predictions = lrModel.transform(testData)\n",
        "  predictions.filter(predictions['prediction'] == 0) \\\n",
        "      .select(\"summary\",\"topic\",\"probability\",\"label\",\"prediction\") \\\n",
        "      .orderBy(\"probability\", ascending=False) \\\n",
        "      .show(n = 10, truncate = 30)\n",
        "\n",
        "  # evaluator_lr_cv = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "  # evaluator_lr_cv.evaluate(predictions)\n",
        "  # Training data f1 score\n",
        "  my_mc_lr = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
        "  F1 = my_mc_lr.evaluate(predictions)\n",
        "\n",
        "\n",
        "  # Training data Accuracy\n",
        "  my_mc_lr = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "  acc = my_mc_lr.evaluate(predictions)\n",
        "  # roc = str(my_mc_lr.evaluate(predictions, {my_mc_lr.metricName: \"areaUnderROC\"}))\n",
        "\n",
        "  # log metrics\n",
        "  mlflow.log_metric(\"F1_score_Training\", F1)\n",
        "  mlflow.log_metric(\"Accuracy_Training\", acc)\n",
        "\n",
        "  runID = run.info.run_uuid\n",
        "  experimentID = run.info.experiment_id\n",
        "\n",
        "  print(\"Inside MLflow Run with run_id {} and experiment_id {}\".format(runID, experimentID))\n",
        "  # mlflow.sklearn.log_model(lrModel, \"lrModel\")\n",
        "  # mlflow.log_params(params)\n",
        "\n",
        "\n",
        "  # print some data\n",
        "  print(\"F1_score_Training\",my_mc_lr.evaluate(predictions))\n",
        "  print(\"Accuracy_Training\",my_mc_lr.evaluate(predictions))\n",
        "  # mlflow.spark.log_model(spark_model=lrModel, sample_input=trainingData, artifact_path=\"model\")\n",
        "\n",
        "  mlflow.spark.save_model(lrModel, \"LR-Model\")\n",
        "# mlflow.spark.load_model(path, run_id=None, dfs_tmpdir='/tmp/mlflow'\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+--------------------+----------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "|sno|               title|             summary|                link|        pub_date|   topic|               words|            filtered|            features|label|\n",
            "+---+--------------------+--------------------+--------------------+----------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "|  0|The difference be...|Business Analysis...|https://medium.co...|16-10-2021 06:09|    news|[business, analys...|[business, analys...|(304,[0,1,2,3,6,8...|  0.0|\n",
            "|  1|Writing Off the E...|Starting a busine...|https://www.theba...|22-10-2021 20:46|    news|[starting, a, bus...|[starting, a, bus...|(304,[0,1,2,4,8,1...|  0.0|\n",
            "|  2|How To Register a...|Finding a busines...|https://blog.hubs...|19-10-2021 11:00|business|[finding, a, busi...|[finding, a, busi...|(304,[0,1,2,4,5,8...|  1.0|\n",
            "|  3|PDF Download*% Es...|Upcoming SlideSha...|https://www.slide...|22-10-2021 03:32|    news|[upcoming, slides...|[upcoming, slides...|(304,[0,1,2,5,6,1...|  0.0|\n",
            "|  4|NBA, LegalZoom Te...|The National Bask...|https://news.yaho...|16-10-2021 00:10|    news|[the, national, b...|[national, basket...|(304,[0,1,2,3,4,5...|  0.0|\n",
            "+---+--------------------+--------------------+--------------------+----------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Training Dataset Count: 89\n",
            "Test Dataset Count: 36\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "|                       summary| topic|                   probability|label|prediction|\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "|Dublin, Oct. 20, 2021 (GLOB...|  news|[0.9891860850294178,0.00234...|  0.0|       0.0|\n",
            "|New York, NY, Oct. 20, 2021...|  news|[0.979586936258292,0.004258...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9788038850670272,0.00547...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9788038850670272,0.00547...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9788038850670272,0.00547...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9781883132175464,0.00559...|  0.0|       0.0|\n",
            "|Upcoming SlideShare What to...|  news|[0.9518927404604928,0.01465...|  0.0|       0.0|\n",
            "|BeLive Technology appoints ...|gaming|[0.9444878753024972,0.01213...|  4.0|       0.0|\n",
            "|Upcoming SlideShare What to...|  news|[0.9248662643557746,0.02506...|  0.0|       0.0|\n",
            "|Upcoming SlideShare What to...|  news|[0.9248662643557746,0.02506...|  0.0|       0.0|\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Inside MLflow Run with run_id 20f4a25a5b1f4d2990d95581c9b06d99 and experiment_id 0\n",
            "F1_score_Training 0.7777777777777778\n",
            "Accuracy_Training 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztEtB2cf7tDZ"
      },
      "source": [
        "<!-- Logistic Regresssion -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZAAICDu-byD"
      },
      "source": [
        "<!-- lOGISTIC REGRESSION IDF -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBIB98Ft7fOZ",
        "outputId": "3e050d77-afdb-44ce-b4e0-b457afcaa070"
      },
      "source": [
        "# Start a new MLflow run\n",
        "import mlflow.mleap\n",
        "import mlflow.sklearn\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "with mlflow.start_run(run_name=\"News_priediction_idf\") as run:\n",
        "    # regular expression tokenizer\n",
        "  regexTokenizer = RegexTokenizer(inputCol=\"summary\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "  # stop words\n",
        "  add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
        "  stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
        "  # bag of words count\n",
        "  countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
        "  label_stringIdx = StringIndexer(inputCol = \"topic\", outputCol = \"label\")\n",
        "  \n",
        "  hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "  idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "  pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
        "  pipelineFit = pipeline.fit(train_df)\n",
        "  dataset = pipelineFit.transform(train_df)\n",
        "  (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "  lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "  lrModel = lr.fit(trainingData)\n",
        "  predictions = lrModel.transform(testData)\n",
        "  predictions.filter(predictions['prediction'] == 0) \\\n",
        "      .select(\"summary\",\"topic\",\"probability\",\"label\",\"prediction\") \\\n",
        "      .orderBy(\"probability\", ascending=False) \\\n",
        "      .show(n = 10, truncate = 30)\n",
        "\n",
        "\n",
        "  # evaluator_lr_cv = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "  # evaluator_lr_cv.evaluate(predictions)\n",
        "  # Training data f1 score\n",
        "  lr_IDF = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
        "  F1 = lr_IDF.evaluate(predictions)\n",
        "\n",
        "\n",
        "  # Training data Accuracy\n",
        "  lr_IDF = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "  acc = lr_IDF.evaluate(predictions)\n",
        "  # roc = str(my_mc_lr.evaluate(predictions, {my_mc_lr.metricName: \"areaUnderROC\"}))\n",
        "\n",
        "  # log metrics\n",
        "  mlflow.log_metric(\"F1_score_Training\", F1)\n",
        "  mlflow.log_metric(\"Accuracy_Training\", acc)\n",
        "\n",
        "  runID = run.info.run_uuid\n",
        "  experimentID = run.info.experiment_id\n",
        "\n",
        "  print(\"Inside MLflow Run with run_id {} and experiment_id {}\".format(runID, experimentID))\n",
        "  # mlflow.sklearn.log_model(lrModel, \"lrModel\")\n",
        "  # mlflow.log_params(params)\n",
        "\n",
        "\n",
        "  # print some data\n",
        "  print(\"F1_score_Training\",F1)\n",
        "  print(\"Accuracy_Training\",acc)\n",
        "  # mlflow.spark.log_model(spark_model=lrModel, sample_input=trainingData, artifact_path=\"model\")\n",
        "\n",
        "  mlflow.spark.save_model(lrModel, \"LR-Model_idf\")\n",
        "# mlflow.spark.load_model(path, run_id=None, dfs_tmpdir='/tmp/mlflow'\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "|                       summary| topic|                   probability|label|prediction|\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "|Dublin, Oct. 20, 2021 (GLOB...|  news|[0.9900824627324072,0.00199...|  0.0|       0.0|\n",
            "|New York, NY, Oct. 20, 2021...|  news|[0.9790350915839408,0.00499...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9782026336033728,0.00617...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9782026336033728,0.00617...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9782026336033728,0.00617...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9772742392270295,0.00634...|  0.0|       0.0|\n",
            "|Upcoming SlideShare What to...|  news|[0.9596441013793896,0.01335...|  0.0|       0.0|\n",
            "|BeLive Technology appoints ...|gaming|[0.9419663928012955,0.00411...|  4.0|       0.0|\n",
            "|HONG KONG, Oct. 19, 2021 /P...|gaming|[0.9087946915111426,0.03209...|  4.0|       0.0|\n",
            "|Upcoming SlideShare What to...|  news|[0.9040172848144538,0.03400...|  0.0|       0.0|\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Inside MLflow Run with run_id 77f31245671247278d5f56b9565f0c05 and experiment_id 0\n",
            "F1_score_Training 0.7457142857142858\n",
            "Accuracy_Training 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REs3ELdFzQCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0b8a5b-2e72-49b2-9c84-4c6ce6477411"
      },
      "source": [
        "# Start a new MLflow run\n",
        "import mlflow.mleap\n",
        "import mlflow.sklearn\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "with mlflow.start_run(run_name=\"News_priediction_cvModel\") as run:\n",
        "  \n",
        "# regular expression tokenizer\n",
        "  regexTokenizer = RegexTokenizer(inputCol=\"summary\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "  # stop words\n",
        "  add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
        "  stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
        "  # bag of words count\n",
        "  countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
        "  label_stringIdx = StringIndexer(inputCol = \"topic\", outputCol = \"label\")\n",
        "\n",
        "  # hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "  # idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "  pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "  pipelineFit = pipeline.fit(train_df)\n",
        "  dataset = pipelineFit.transform(train_df)\n",
        "  (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "  lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        " \n",
        "  evaluator_lr_tf = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "  # Create ParamGrid for Cross Validation\n",
        "  paramGrid = (ParamGridBuilder()\n",
        "              .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
        "              .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
        "  #            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
        "  #            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
        "              .build())\n",
        "  # Create 5-fold CrossValidator\n",
        "  cv = CrossValidator(estimator=lr, \\\n",
        "                      estimatorParamMaps=paramGrid, \\\n",
        "                      evaluator=evaluator_lr_tf, \\\n",
        "                      numFolds=5)\n",
        "  cvModel = cv.fit(trainingData)\n",
        "\n",
        "  predictions = cvModel.transform(testData)\n",
        "  # Evaluate best model\n",
        "  evaluator_cv = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "  evaluator_cv.evaluate(predictions)\n",
        "  runID = run.info.run_uuid\n",
        "  experimentID = run.info.experiment_id\n",
        "\n",
        "  print(\"Inside MLflow Run with run_id {} and experiment_id {}\".format(runID, experimentID))\n",
        "  # mlflow.sklearn.log_model(lrModel, \"lrModel\")\n",
        "  # mlflow.log_params(params)\n",
        "  # Training data f1 score\n",
        "  evaluator_cv = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
        "  F1 = evaluator_cv.evaluate(predictions)\n",
        "\n",
        "\n",
        "  # Training data Accuracy\n",
        "  evaluator_cv = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "  acc = evaluator_cv.evaluate(predictions)\n",
        "\n",
        "\n",
        "  # print some data\n",
        "  print(\"F1_score_Training\",F1)\n",
        "  print(\"Accuracy_Training\",acc)\n",
        "  # mlflow.spark.log_model(spark_model=lrModel, sample_input=trainingData, artifact_path=\"model\")\n",
        "\n",
        "  mlflow.spark.save_model(cvModel, \"cvModel\")\n",
        "# mlflow.spark.load_model(path, run_id=None, dfs_tmpdir='/tmp/mlflow'\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside MLflow Run with run_id a6fe024c1cb3480e88ad8b95a9f40286 and experiment_id 0\n",
            "F1_score_Training 0.7457142857142858\n",
            "Accuracy_Training 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xchmxy8ACG7m"
      },
      "source": [
        "<!-- Naive Bayes -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UCfRfQzCBxf",
        "outputId": "13c6d4cb-0b76-4caf-85c1-515d3eabb46f"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "with mlflow.start_run(run_name=\"News_priediction_naive\") as run:\n",
        "  nb = NaiveBayes(smoothing=1)\n",
        "  model = nb.fit(trainingData)\n",
        "  predictions = model.transform(testData)\n",
        "  predictions.filter(predictions['prediction'] == 0) \\\n",
        "      .select(\"summary\",\"topic\",\"probability\",\"label\",\"prediction\") \\\n",
        "      .orderBy(\"probability\", ascending=False) \\\n",
        "      .show(n = 10, truncate = 30)\n",
        "  evaluator_nb = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "  evaluator_nb.evaluate(predictions)\n",
        "  evaluator_nb = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
        "  F1 = evaluator_nb.evaluate(predictions)\n",
        "\n",
        "\n",
        "  # Training data Accuracy\n",
        "  evaluator_nb = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "  acc = evaluator_nb.evaluate(predictions)\n",
        "\n",
        "\n",
        "  # print some data\n",
        "  print(\"F1_score_Training\",F1)\n",
        "  print(\"Accuracy_Training\",acc)\n",
        "  mlflow.spark.save_model(model, \"nbModel\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "|                       summary| topic|                   probability|label|prediction|\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "|Home News Sports Living Art...|  news|[0.999999999985695,2.832303...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.999999999985695,2.832303...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.999999999985695,2.832303...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.9999999986664769,1.34168...|  0.0|       0.0|\n",
            "|BeLive Technology appoints ...|gaming|[0.9999990856843528,1.93480...|  4.0|       0.0|\n",
            "|After months of intense and...|  news|[0.9999982673093013,5.51076...|  0.0|       0.0|\n",
            "|Dublin, Oct. 20, 2021 (GLOB...|  news|[0.9999958679216261,5.53052...|  0.0|       0.0|\n",
            "|International Game Technolo...|  news|[0.9999842407380422,6.33079...|  0.0|       0.0|\n",
            "|New York, NY, Oct. 20, 2021...|  news|[0.999958819524484,1.500015...|  0.0|       0.0|\n",
            "|By Sun Xiaochen | China Dai...|  news|[0.9998534872205989,5.32532...|  0.0|       0.0|\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "F1_score_Training 0.7095959595959596\n",
            "Accuracy_Training 0.7222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD7MU3gmCMe0",
        "outputId": "9991e20b-2832-4225-e07d-d757e625cb6b"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "with mlflow.start_run(run_name=\"News_priediction_RF\") as run:\n",
        "  rf = RandomForestClassifier(labelCol=\"label\", \\\n",
        "                              featuresCol=\"features\", \\\n",
        "                              numTrees = 100, \\\n",
        "                              maxDepth = 4, \\\n",
        "                              maxBins = 32)\n",
        "  # Train model with Training Data\n",
        "  rfModel = rf.fit(trainingData)\n",
        "  predictions = rfModel.transform(testData)\n",
        "  predictions.filter(predictions['prediction'] == 0) \\\n",
        "      .select(\"summary\",\"topic\",\"probability\",\"label\",\"prediction\") \\\n",
        "      .orderBy(\"probability\", ascending=False) \\\n",
        "      .show(n = 10, truncate = 30)\n",
        "  evaluator_rf = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "  evaluator_rf.evaluate(predictions)\n",
        "  evaluator_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
        "  F1 = evaluator_rf.evaluate(predictions)\n",
        "\n",
        "\n",
        "  # Training data Accuracy\n",
        "  evaluator_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "  acc = evaluator_rf.evaluate(predictions)\n",
        "\n",
        "\n",
        "  # print some data\n",
        "  print(\"F1_score_Training\",F1)\n",
        "  print(\"Accuracy_Training\",acc)\n",
        "  mlflow.spark.save_model(rfModel, \"RFModel\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "|                       summary| topic|                   probability|label|prediction|\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "|Home News Sports Living Art...|  news|[0.8337578294853107,0.04293...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.8337578294853107,0.04293...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.8337578294853107,0.04293...|  0.0|       0.0|\n",
            "|Home News Sports Living Art...|  news|[0.8337578294853107,0.04293...|  0.0|       0.0|\n",
            "|New York, NY, Oct. 20, 2021...|  news|[0.7579930640065959,0.08359...|  0.0|       0.0|\n",
            "|Press release content from ...|  tech|[0.7542430918118623,0.08007...|  2.0|       0.0|\n",
            "|Dublin, Oct. 20, 2021 (GLOB...|  news|[0.749597314383296,0.069138...|  0.0|       0.0|\n",
            "|HONG KONG, Oct. 19, 2021 /P...|gaming|[0.7415773656276062,0.08556...|  4.0|       0.0|\n",
            "|After months of intense and...|  news|[0.7383922497601421,0.08947...|  0.0|       0.0|\n",
            "|Upcoming SlideShare What to...|  news|[0.724545992096354,0.080088...|  0.0|       0.0|\n",
            "+------------------------------+------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "F1_score_Training 0.6183908045977011\n",
            "Accuracy_Training 0.7222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zb_S0xEEQxM",
        "outputId": "a6dfd600-d270-4ae5-fd40-4a0046e26902"
      },
      "source": [
        "# Accuracy\n",
        "\n",
        "# print(\"Accuracy of LR using Count Vectorizer\", my_mc_lr.evaluate(predictions))\n",
        "# print(\"Accuracy of LR using IDF\", evaluator_idf.evaluate(predictions))\n",
        "# print(\"Accuracy of LR using cross validation and hyperparameters\", evaluator_cv.evaluate(predictions))\n",
        "# print(\"Accuracy of Naive Bayes\", evaluator_nb.evaluate(predictions))\n",
        "# print(\"Accuracy of Random Forest\", evaluator_rf.evaluate(predictions))\n",
        "# print(\"Accuracy of LR using Count Vectorizer\", my_mc_lr.evaluate(predictions))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of LR using Count Vectorizer 0.7222222222222222\n",
            "Accuracy of LR using IDF 0.7222222222222222\n",
            "Accuracy of LR using cross validation and hyperparameters 0.7222222222222222\n",
            "Accuracy of Naive Bayes 0.7222222222222222\n",
            "Accuracy of Random Forest 0.7222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyE2YDp9zrpl",
        "outputId": "b31c7546-945d-48d8-cb95-10b398a16122"
      },
      "source": [
        "# Enter your auth token when the code is running\n",
        "# 1zt2ytXyrAhXfMSFbjJBkqbCmD6_5FEDkfjsNCearfyE3i2JD\n",
        "NGROK_AUTH_TOKEN = getpass('Enter the ngrok authtoken:')\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the ngrok authtoken:··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t=2021-10-24T09:49:08+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking UI: https://9f08-35-197-30-160.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZEcjFuh2aYI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98bb9f8f-c0ea-4ece-ec6b-9b6679fb2cae"
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeicrjLqu0CU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}